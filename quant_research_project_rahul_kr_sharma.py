# -*- coding: utf-8 -*-
"""Quant_research_Project_Rahul Kr Sharma.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1q_2UVE35HMcNmHj0Db7iIuV3AjauGX5b

"With 6 years of experience in the stock market, I specialize in trading options and investing in the cash market. I'm skilled at using Zerodha platform for trading. I've successfully managed investments by analyzing market trends and making smart decisions."
"""

# import the library
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
import seaborn as sns

# load the dataset
data=pd.read_parquet('/content/data.parquet')
print(data.head(10))

# Convert timestamp column to datetime index
data_index = pd.to_datetime(data.index)
# Rename the timestamp index to 'Datetime'
data_index.name = 'Date time'
print(data.head(5))
# Filter data within trading hours
data = data.between_time('09:15', '15:30')

# Check for missing values
missing_values = data.isnull().sum()
# Drop rows with missing values
data.dropna(inplace=True)  # Remove inplace=True from the assignment
data.fillna(0, inplace=True)

# calculate the spread
data['spread']= data['banknifty'] - data['nifty']

# calculate the P/L
data['P/L'] = data['spread'] * (data['tte'] ** 0.7)

# compute z scores of the spread using historical data
data["z_score"]=(data["spread"]-data["spread"].mean())/data["spread"].std()


# plot different types of curves
plt.figure(figsize=(12,8))

# time series of banknifty and nifty
plt.subplot(3,1,1)
plt.plot(data.index,data["banknifty"],label="banknifty")
plt.plot(data.index,data["nifty"],label="nifty")
plt.title("time series of banknifty and nifty")
plt.xlabel("time")
plt.ylabel("implied volatility")
plt.legend()
plt.show()

# spread over time
plt.subplot(3,1,1)
plt.plot(data.index,data["spread"],color="orange")
plt.title("spread between banknifty and nifty")
plt.xlabel("Date")
plt.ylabel("spread")
plt.show()

# plot z score curve
plt.figure(figsize=(10,6))
plt.plot(data.index,data['z_score'] ,color='pink')
plt.title("Z-score of spread between banknity and nifty")
plt.xlabel("Date")
plt.ylabel("Z_score")
plt.grid(True)

plt.tight_layout()
plt.show()

# Define thresholds for trading signals
buy_signals = -1
sell_signals = 1

# Generate trading signals
data['signals'] = 0
data.loc[data['z_score'] < buy_signals, 'signals'] = 1 # Buy signal
data.loc[data['z_score'] > sell_signals, 'signals'] = -1 # Sell signal

# Calculate P/L based on trading signals
data['P/L'] = data['spread'] * data['signals'].shift(1) * (data['tte'] ** 0.7)

# Performance Evaluation
def calculate_performance_metrics(data):
    total_pnl = data['P/L'].sum()
    sharpe_ratio = data['P/L'].mean() / data['P/L'].std()

    cum_return = data['P/L'].cumsum()
    peak = cum_return.expanding(min_periods=1).max()
    drawdown = (cum_return - peak) / peak
    drawdown_without_inf = drawdown[peak != 0]
    max_drawdown = drawdown_without_inf.min()

    return total_pnl, sharpe_ratio, max_drawdown

# Calculate Performance Metrics
total_pnl, sharpe_ratio, max_drawdown = calculate_performance_metrics(data)

# Output Performance Metrics
print("Base model Evaluation")
print("Total P/L:", total_pnl)
print("Sharpe ratio:", sharpe_ratio)
print("Max Drawdown:", max_drawdown)

# plot z score curve
plt.figure(figsize=(12,6))
plt.plot(data.index,data['z_score'],color='red')
plt.axhline(buy_signals, color='green', linestyle='--', label='Buy Threshold')
plt.axhline(sell_signals, color='red', linestyle='--', label='Sell Threshold')
plt.title("Z-score of spread between banknity and nifty")
plt.xlabel("date")
plt.ylabel("Z_score")
plt.grid(True)
plt.tight_layout()
plt.show()

# Check for outliers using box plot
plt.figure(figsize=(8, 6))
sns.boxplot(data['spread'], color='green')
plt.title('Box Plot of Spread')
plt.xlabel('Spread')
plt.show()
# Histogram of spread distribution
plt.figure(figsize=(8, 6))
sns.histplot(data['spread'], kde=True, color='orange')
plt.title('Histogram of Spread Distribution')
plt.xlabel('Spread')
plt.ylabel('Frequency')
plt.show()
# Check for outliers using box plot
plt.figure(figsize=(8, 6))
sns.boxplot(data['P/L'], color='green')
plt.title('Box Plot of P/L')
plt.xlabel('P/L')
plt.show()

# Histogram of profit and loss
plt.figure(figsize=(8, 6))
sns.histplot(data['P/L'], kde=True, color='green')
plt.xlabel('P/L')
plt.title('Histogram of profit and loss')
plt.ylabel('Frequency')
plt.show()

# Check for outliers using box plot
plt.figure(figsize=(8, 6))
sns.boxplot(data["z_score"] , color='green')
plt.title('Box Plot of Spread')
plt.xlabel('z_score')
plt.show()

# Histogram of Z-SCORES distribution
plt.figure(figsize=(8, 6))
sns.histplot(data["z_score"] , kde=True, color='orange')
plt.title('Histogram of Z-SCORES Distribution')
plt.xlabel('z_score')
plt.ylabel('Frequency')
plt.show()

from sklearn.ensemble import GradientBoostingRegressor
from sklearn.metrics import mean_squared_error
from sklearn.preprocessing import StandardScaler

# Define features and target variable

X = data[['banknifty','nifty', 'tte']]  # Add additional features as needed
y =data['signals']

# Fill NaN with zero
data.fillna(0, inplace=True)
# Normalize features
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Split the data into training and testing sets
train_size = int(len(data) * 0.8)
train_X, test_X = X_scaled[:train_size], X_scaled[train_size:]
train_y, test_y = y[:train_size], y[train_size:]

# Train Gradient Boosting Machine model
gbm_model = GradientBoostingRegressor()
gbm_model.fit(train_X, train_y)

# Predict P/L for the test set
predicted_pl_gbm = gbm_model.predict(test_X)

# Function to calculate Sharpe Ratio
def calculate_sharpe_ratio(pl_series):
    return np.mean(pl_series) / np.std(pl_series)
sharpe_ratio_gbm = calculate_sharpe_ratio(predicted_pl_gbm)

# Function to calculate Drawdown
def calculate_drawdown(pl_series):
    max_drawdown = 0
    max_value = pl_series[0]
    for value in pl_series:
        if value > max_value:
            max_value = value
        drawdown = (max_value - value) / max_value
        if drawdown > max_drawdown:
            max_drawdown = drawdown
    return max_drawdown

# Evaluate model performance
sharpe_ratio_gbm = calculate_sharpe_ratio(predicted_pl_gbm)
drawdown_gbm = calculate_drawdown(np.cumsum(predicted_pl_gbm))

print("\nGradient Boosting Machine Model:")
print("Absolute P/L:", np.sum(predicted_pl_gbm))
print("Sharpe Ratio:", sharpe_ratio_gbm)
print("Drawdown:", drawdown_gbm)

from sklearn.model_selection import RandomizedSearchCV
from scipy.stats import randint, uniform

# Define the parameter grid
param_grid = {
    'n_estimators': randint(5,9 ),  # Number of boosting stages to be run
    'learning_rate': uniform(0.01, 0.3),  # Learning rate shrinks the contribution of each tree
    'max_depth': randint(3, 10),  # Maximum depth of the individual estimators
    'min_samples_split': randint(2, 20),  # Minimum number of samples required to split an internal node
    'min_samples_leaf': randint(1, 10)  # Minimum number of samples required to be at a leaf node
}

# Initialize Gradient Boosting Machine model
gbm_model = GradientBoostingRegressor()

# Initialize RandomizedSearchCV
random_search = RandomizedSearchCV(gbm_model, param_distributions=param_grid, n_iter=50, cv=5, scoring='neg_mean_squared_error', random_state=42)

# Perform randomized search
random_search.fit(train_X, train_y)

# Get the best parameters
best_params = random_search.best_params_

# Train Gradient Boosting Machine model with the best parameters
gbm_model_best = GradientBoostingRegressor(**best_params)
gbm_model_best.fit(train_X, train_y)

# Predict P/L for the test set using the best model
predicted_pl_gbm_best = gbm_model_best.predict(test_X)

# Evaluate model performance
sharpe_ratio_gbm_best = calculate_sharpe_ratio(predicted_pl_gbm_best)
drawdown_gbm_best = calculate_drawdown(np.cumsum(predicted_pl_gbm_best))

print("\nGradient Boosting Machine Model with RandomizedSearchCV Best Parameters:")
print("Absolute P/L:", np.sum(predicted_pl_gbm_best))
print("Sharpe Ratio:", sharpe_ratio_gbm_best)
print("Drawdown:", drawdown_gbm_best)

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense
from tensorflow.keras.optimizers import Adam

# Reshape data for LSTM input (assuming a time series structure)
train_X_lstm = train_X.reshape(train_X.shape[0], 1, train_X.shape[1])
test_X_lstm = test_X.reshape(test_X.shape[0], 1, test_X.shape[1])

# Build LSTM model
lstm_model = Sequential()
lstm_model.add(LSTM(units=50, input_shape=(train_X_lstm.shape[1], train_X_lstm.shape[2])))
lstm_model.add(Dense(units=1))
lstm_model.compile(optimizer=Adam(learning_rate=0.001), loss='mean_squared_error') # we also calculate Hubber loss, KL div --> Regression

# Train LSTM model
lstm_model.fit(train_X_lstm, train_y, epochs=5, batch_size=32, verbose=1)

# Predict P/L for the test set using LSTM model
predicted_pl_lstm = lstm_model.predict(test_X_lstm).reshape(-1)

# Calculate Sharpe Ratio and Drawdown for LSTM model
sharpe_ratio_lstm = calculate_sharpe_ratio(predicted_pl_lstm)
drawdown_lstm = calculate_drawdown(np.cumsum(predicted_pl_lstm))

print("\nLong Short-Term Memory (LSTM) Model:")
print("Absolute P/L:", np.sum(predicted_pl_lstm))
print("Sharpe Ratio:", sharpe_ratio_lstm)
print("Drawdown:", drawdown_lstm)

"""**Summary of results and findings.**

we implemented and compared two trading models against a base model that uses z-scores of the spread to identify trading opportunities. The goal was to optimize the absolute P/L, Sharpe Ratio, and Drawdown of the strategies.

The first model we developed was a Gradient Boosting Machine (GBM) model. It outperformed the base model, resulting in a positive absolute P/L and a higher Sharpe Ratio. However, there was still room for improvement.

To further enhance the GBM model, we employed RandomizedSearchCV to find the best hyperparameters. This optimization led to an increase in both absolute P/L and Sharpe Ratio while reducing the Drawdown, indicating a more robust and efficient trading strategy.
we explored the Long Short-Term Memory (LSTM) model, a type of recurrent neural network (RNN) commonly used in time-series forecasting tasks. The LSTM model performed exceptionally well, achieving the highest absolute P/L, Sharpe Ratio, and the lowest Drawdown among all models tested.

**Report:**

Base Model Evaluation:

Total P/L: -16092.085493939467

Sharpe Ratio: -0.25438400925635074

Max Drawdown: -142.37087177125255

Gradient Boosting Machine Model:

Absolute P/L: 8492.168050030688

Sharpe Ratio: 0.7855494810945853

Drawdown: 0.15791623592376758

Gradient Boosting Machine Model with RandomizedSearchCV Best Parameters:

Absolute P/L: 13216.071132532015

Sharpe Ratio: 0.7846196365061618

Drawdown: 0.026385447038059007

Long Short-Term Memory (LSTM) Model:

Absolute P/L: 12232.357

Sharpe Ratio: 0.9868196

Drawdown: 0.0065799835

Comparison:

The base model using z-scores performed poorly, resulting in negative absolute P/L and a negative Sharpe Ratio.
The Gradient Boosting Machine model showed improvement in absolute P/L and Sharpe Ratio compared to the base model, but it still had a relatively high Drawdown.
Optimizing the GBM model with RandomizedSearchCV significantly enhanced performance, increasing both absolute P/L and Sharpe Ratio while minimizing Drawdown.
"""